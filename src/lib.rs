// src/lib.rs - Enhanced multi-core backend with optimization support
#![deny(clippy::all)]

use napi_derive::napi;
use napi::Result;
use std::sync::{ Arc, Mutex };
use std::thread;
use crossbeam::channel;
use serde::{ Deserialize, Serialize };
use std::time::{ Duration, Instant };
use std::collections::HashMap;

/// Enhanced task with optimization metadata
#[derive(Debug, Clone)]
pub struct OptimizedWorkerTask {
  pub id: String,
  pub function_code: String,
  pub data: String,
  pub timeout_ms: Option<u64>,
  pub optimization_hints: OptimizationHints,
}

/// Optimization hints from the preparation phase
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OptimizationHints {
  pub operation_type: String, // "mathematical", "string_operations", etc.
  pub complexity: String, // "low", "medium", "high"
  pub expected_cores: u32,
  pub should_use_rust: bool,
  pub is_hot_function: bool,
  pub estimated_memory: u64,
  pub batch_size_hint: Option<u32>,
  pub function_hash: String,
}

/// Enhanced result with optimization metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OptimizedWorkerResult {
  pub id: String,
  pub result: Option<String>,
  pub error: Option<String>,
  pub duration_ms: u64,
  pub worker_id: usize,
  pub execution_type: String,
  pub optimization_used: String, // "rust_native", "rust_optimized", "needs_js_worker"
  pub cache_hit: bool,
}

/// Performance tracking for optimization learning
#[derive(Debug, Clone)]
struct PerformanceMetric {
  function_hash: String,
  execution_time: u64,
  throughput: f64,
  optimization_type: String,
  success: bool,
}

/// Enhanced multi-core executor with optimization capabilities
#[napi]
pub struct OptimizedMultiCoreExecutor {
  worker_count: usize,
  task_sender: Arc<Mutex<Option<channel::Sender<OptimizedWorkerTask>>>>,
  result_receiver: Arc<Mutex<Option<channel::Receiver<OptimizedWorkerResult>>>>,
  workers_active: Arc<Mutex<bool>>,

  // Optimization state
  performance_cache: Arc<Mutex<HashMap<String, PerformanceMetric>>>,
  hot_functions: Arc<Mutex<HashMap<String, u32>>>,
  optimization_stats: Arc<Mutex<OptimizationStats>>,
  warmed_cores: Arc<Mutex<Vec<bool>>>,
}

#[derive(Debug, Clone)]
struct OptimizationStats {
  rust_native_hits: u64,
  js_worker_fallbacks: u64,
  cache_hits: u64,
  total_executions: u64,
  avg_latency_ms: f64,
  throughput_tasks_per_sec: f64,
}

impl Default for OptimizationStats {
  fn default() -> Self {
    Self {
      rust_native_hits: 0,
      js_worker_fallbacks: 0,
      cache_hits: 0,
      total_executions: 0,
      avg_latency_ms: 0.0,
      throughput_tasks_per_sec: 0.0,
    }
  }
}

#[napi]
impl OptimizedMultiCoreExecutor {
  /// Create enhanced executor with optimization capabilities
  #[napi(constructor)]
  pub fn new(worker_count: Option<u32>) -> Result<Self> {
    let cores = worker_count.unwrap_or_else(|| num_cpus::get() as u32) as usize;

    let (task_tx, task_rx) = channel::unbounded::<OptimizedWorkerTask>();
    let (result_tx, result_rx) = channel::unbounded::<OptimizedWorkerResult>();

    let task_rx = Arc::new(Mutex::new(task_rx));
    let result_tx = Arc::new(Mutex::new(result_tx));
    let workers_active = Arc::new(Mutex::new(true));

    // Initialize optimization state
    let performance_cache = Arc::new(Mutex::new(HashMap::new()));
    let hot_functions = Arc::new(Mutex::new(HashMap::new()));
    let optimization_stats = Arc::new(Mutex::new(OptimizationStats::default()));
    let warmed_cores = Arc::new(Mutex::new(vec![false; cores]));

    // Spawn optimized worker threads
    for worker_id in 0..cores {
      let task_rx = Arc::clone(&task_rx);
      let result_tx = Arc::clone(&result_tx);
      let workers_active = Arc::clone(&workers_active);
      let performance_cache = Arc::clone(&performance_cache);
      let hot_functions = Arc::clone(&hot_functions);

      thread::spawn(move || {
        Self::optimized_worker_thread(
          worker_id,
          task_rx,
          result_tx,
          workers_active,
          performance_cache,
          hot_functions
        );
      });
    }

    Ok(Self {
      worker_count: cores,
      task_sender: Arc::new(Mutex::new(Some(task_tx))),
      result_receiver: Arc::new(Mutex::new(Some(result_rx))),
      workers_active,
      performance_cache,
      hot_functions,
      optimization_stats,
      warmed_cores,
    })
  }

  /// Warm up specific cores for expected workload
  #[napi]
  pub fn warm_cores_for_workload(&self, operation_type: String, expected_cores: u32) -> Result<()> {
    if let Ok(mut warmed) = self.warmed_cores.lock() {
      for i in 0..(expected_cores as usize).min(warmed.len()) {
        warmed[i] = true;
      }
    }

    println!("ðŸ”¥ Warmed {} cores for {} workload", expected_cores, operation_type);
    Ok(())
  }

  /// Submit optimized task with preparation hints
  #[napi]
  pub fn submit_optimized_task(
    &self,
    function_code: String,
    data: String,
    optimization_hints: String // JSON serialized OptimizationHints
  ) -> Result<String> {
    let task_id = Self::generate_task_id();

    // Parse optimization hints
    let hints: OptimizationHints = serde_json
      ::from_str(&optimization_hints)
      .map_err(|e| napi::Error::from_reason(format!("Invalid optimization hints: {}", e)))?;

    let task = OptimizedWorkerTask {
      id: task_id.clone(),
      function_code,
      data,
      timeout_ms: None,
      optimization_hints: hints,
    };

    if let Ok(sender_guard) = self.task_sender.lock() {
      if let Some(sender) = sender_guard.as_ref() {
        sender.send(task).map_err(|e| napi::Error::from_reason(e.to_string()))?;
        Ok(task_id)
      } else {
        Err(napi::Error::from_reason("Worker pool is shut down".to_string()))
      }
    } else {
      Err(napi::Error::from_reason("Failed to access task sender".to_string()))
    }
  }

  /// Submit optimized batch with adaptive sizing
  #[napi]
  pub fn submit_optimized_batch(
    &self,
    tasks: Vec<(String, String)>, // (function_code, data)
    optimization_hints: String // JSON serialized OptimizationHints
  ) -> Result<Vec<String>> {
    let mut task_ids = Vec::new();

    // Parse optimization hints
    let hints: OptimizationHints = serde_json
      ::from_str(&optimization_hints)
      .map_err(|e| napi::Error::from_reason(format!("Invalid optimization hints: {}", e)))?;

    if let Ok(sender_guard) = self.task_sender.lock() {
      if let Some(sender) = sender_guard.as_ref() {
        for (function_code, data) in tasks {
          let task_id = Self::generate_task_id();

          let task = OptimizedWorkerTask {
            id: task_id.clone(),
            function_code,
            data,
            timeout_ms: None,
            optimization_hints: hints.clone(),
          };

          sender.send(task).map_err(|e| napi::Error::from_reason(e.to_string()))?;
          task_ids.push(task_id);
        }
        Ok(task_ids)
      } else {
        Err(napi::Error::from_reason("Worker pool is shut down".to_string()))
      }
    } else {
      Err(napi::Error::from_reason("Failed to access task sender".to_string()))
    }
  }

  /// Get optimized result with performance metrics
  #[napi]
  pub fn get_optimized_result(&self, timeout_ms: Option<u32>) -> Result<String> {
    if let Ok(receiver_guard) = self.result_receiver.lock() {
      if let Some(receiver) = receiver_guard.as_ref() {
        let result = if let Some(timeout) = timeout_ms {
          receiver
            .recv_timeout(Duration::from_millis(timeout as u64))
            .map_err(|e| napi::Error::from_reason(e.to_string()))?
        } else {
          receiver.recv().map_err(|e| napi::Error::from_reason(e.to_string()))?
        };

        // Update optimization stats
        self.update_optimization_stats(&result);

        serde_json::to_string(&result).map_err(|e| napi::Error::from_reason(e.to_string()))
      } else {
        Err(napi::Error::from_reason("Worker pool is shut down".to_string()))
      }
    } else {
      Err(napi::Error::from_reason("Failed to access result receiver".to_string()))
    }
  }

  /// Get batch results with optimization tracking
  #[napi]
  pub fn get_optimized_batch_results(
    &self,
    task_count: u32,
    timeout_ms: Option<u32>
  ) -> Result<Vec<String>> {
    let mut results = Vec::new();
    let deadline = timeout_ms.map(|t| Instant::now() + Duration::from_millis(t as u64));
    let batch_start = Instant::now();

    if let Ok(receiver_guard) = self.result_receiver.lock() {
      if let Some(receiver) = receiver_guard.as_ref() {
        for _ in 0..task_count {
          let remaining_time = deadline.map(|d| d.saturating_duration_since(Instant::now()));

          let result = if let Some(timeout) = remaining_time {
            if timeout.is_zero() {
              return Err(napi::Error::from_reason("Batch timeout exceeded".to_string()));
            }
            receiver.recv_timeout(timeout).map_err(|e| napi::Error::from_reason(e.to_string()))?
          } else {
            receiver.recv().map_err(|e| napi::Error::from_reason(e.to_string()))?
          };

          // Update optimization stats for each result
          self.update_optimization_stats(&result);

          let result_json = serde_json
            ::to_string(&result)
            .map_err(|e| napi::Error::from_reason(e.to_string()))?;
          results.push(result_json);
        }

        // Update batch performance metrics
        let batch_duration = batch_start.elapsed().as_millis() as f64;
        let throughput = ((task_count as f64) / batch_duration) * 1000.0;

        if let Ok(mut stats) = self.optimization_stats.lock() {
          stats.throughput_tasks_per_sec = throughput;
        }

        Ok(results)
      } else {
        Err(napi::Error::from_reason("Worker pool is shut down".to_string()))
      }
    } else {
      Err(napi::Error::from_reason("Failed to access result receiver".to_string()))
    }
  }

  /// Get optimization performance statistics
  #[napi]
  pub fn get_optimization_stats(&self) -> Result<String> {
    if let Ok(stats) = self.optimization_stats.lock() {
      let stats_json =
        serde_json::json!({
                "rust_native_hits": stats.rust_native_hits,
                "js_worker_fallbacks": stats.js_worker_fallbacks,
                "cache_hits": stats.cache_hits,
                "total_executions": stats.total_executions,
                "avg_latency_ms": stats.avg_latency_ms,
                "throughput_tasks_per_sec": stats.throughput_tasks_per_sec,
                "optimization_ratio": if stats.total_executions > 0 {
                    stats.rust_native_hits as f64 / stats.total_executions as f64
                } else { 0.0 },
                "cache_hit_ratio": if stats.total_executions > 0 {
                    stats.cache_hits as f64 / stats.total_executions as f64
                } else { 0.0 }
            });

      Ok(stats_json.to_string())
    } else {
      Err(napi::Error::from_reason("Failed to access optimization stats".to_string()))
    }
  }

  /// Clear optimization caches and reset stats
  #[napi]
  pub fn reset_optimization_state(&self) -> Result<()> {
    if let Ok(mut cache) = self.performance_cache.lock() {
      cache.clear();
    }

    if let Ok(mut hot_funcs) = self.hot_functions.lock() {
      hot_funcs.clear();
    }

    if let Ok(mut stats) = self.optimization_stats.lock() {
      *stats = OptimizationStats::default();
    }

    if let Ok(mut warmed) = self.warmed_cores.lock() {
      warmed.fill(false);
    }

    Ok(())
  }

  /// Enhanced worker thread with optimization awareness
  fn optimized_worker_thread(
    worker_id: usize,
    task_receiver: Arc<Mutex<channel::Receiver<OptimizedWorkerTask>>>,
    result_sender: Arc<Mutex<channel::Sender<OptimizedWorkerResult>>>,
    workers_active: Arc<Mutex<bool>>,
    performance_cache: Arc<Mutex<HashMap<String, PerformanceMetric>>>,
    hot_functions: Arc<Mutex<HashMap<String, u32>>>
  ) {
    while (
      {
        if let Ok(active) = workers_active.lock() { *active } else { false }
      }
    ) {
      let task = {
        if let Ok(receiver_guard) = task_receiver.lock() {
          receiver_guard.try_recv()
        } else {
          break;
        }
      };

      match task {
        Ok(task) => {
          let start_time = Instant::now();

          // Check cache for hot functions
          let cache_hit = if task.optimization_hints.is_hot_function {
            if let Ok(cache) = performance_cache.lock() {
              cache.contains_key(&task.optimization_hints.function_hash)
            } else {
              false
            }
          } else {
            false
          };

          let (result, optimization_used) = if task.optimization_hints.should_use_rust {
            // Use enhanced Rust native execution
            match Self::execute_rust_optimized(&task, worker_id) {
              Ok(result) => (Ok(result), "rust_optimized".to_string()),
              Err(err) => (Err(err), "rust_failed".to_string()),
            }
          } else {
            // Signal for JavaScript worker with optimization hints
            (Err("NEEDS_JS_WORKER_OPTIMIZED".to_string()), "needs_js_worker".to_string())
          };

          let duration = start_time.elapsed().as_millis() as u64;

          // Update hot function tracking
          if let Ok(mut hot_funcs) = hot_functions.lock() {
            let count = hot_funcs.entry(task.optimization_hints.function_hash.clone()).or_insert(0);
            *count += 1;
          }

          // Record performance metric
          if let Ok(mut cache) = performance_cache.lock() {
            let metric = PerformanceMetric {
              function_hash: task.optimization_hints.function_hash.clone(),
              execution_time: duration,
              throughput: 1000.0 / (duration as f64),
              optimization_type: optimization_used.clone(),
              success: result.is_ok(),
            };
            cache.insert(task.optimization_hints.function_hash.clone(), metric);
          }

          let (result_ok, result_err) = match result {
            Ok(val) => (Some(val), None),
            Err(err) => (None, Some(err)),
          };

          let worker_result = OptimizedWorkerResult {
            id: task.id,
            result: result_ok,
            error: result_err,
            duration_ms: duration,
            worker_id,
            execution_type: optimization_used.clone(),
            optimization_used,
            cache_hit,
          };

          if let Ok(sender_guard) = result_sender.lock() {
            let _ = sender_guard.send(worker_result);
          }
        }
        Err(channel::TryRecvError::Empty) => {
          thread::sleep(Duration::from_millis(1));
        }
        Err(channel::TryRecvError::Disconnected) => {
          break;
        }
      }
    }
  }

  /// Enhanced Rust native execution with optimization hints
  fn execute_rust_optimized(
    task: &OptimizedWorkerTask,
    worker_id: usize
  ) -> std::result::Result<String, String> {
    let parsed_data: serde_json::Value = match serde_json::from_str(&task.data) {
      Ok(val) => val,
      Err(_) => serde_json::Value::String(task.data.clone()),
    };

    // Use optimization hints for better performance
    match task.optimization_hints.operation_type.as_str() {
      "mathematical" =>
        Self::execute_mathematical_optimized(&task.function_code, &parsed_data, worker_id),
      "string_operations" => Self::execute_string_optimized(&task.function_code, &parsed_data),
      "array_operations" => Self::execute_array_optimized(&task.function_code, &parsed_data),
      _ => Self::execute_general_optimized(&task.function_code, &parsed_data, worker_id),
    }
  }

  /// Optimized mathematical operations
  fn execute_mathematical_optimized(
    function_code: &str,
    data: &serde_json::Value,
    worker_id: usize
  ) -> std::result::Result<String, String> {
    let clean_fn = function_code.replace([' ', '\n', '\t'], "").to_lowercase();

    if let Some(num) = data.as_f64() {
      let result = if clean_fn.contains("*2") {
        num * 2.0
      } else if clean_fn.contains("+5") {
        num + 5.0
      } else if clean_fn.contains("+10") {
        num + 10.0
      } else if clean_fn.contains("*x") || clean_fn.contains("x*x") {
        num * num
      } else if clean_fn.contains("sin") {
        num.sin()
      } else if clean_fn.contains("cos") {
        num.cos()
      } else if clean_fn.contains("sqrt") {
        num.sqrt()
      } else {
        return Err("Unsupported mathematical operation".to_string());
      };

      return Ok(result.to_string());
    }

    // Handle mathematical objects
    if let Some(obj) = data.as_object() {
      if
        let (Some(a), Some(b)) = (
          obj.get("a").and_then(|v| v.as_f64()),
          obj.get("b").and_then(|v| v.as_f64()),
        )
      {
        let result = if clean_fn.contains("*") {
          a * b
        } else if clean_fn.contains("+") {
          a + b
        } else if clean_fn.contains("-") {
          a - b
        } else if clean_fn.contains("/") && b != 0.0 {
          a / b
        } else {
          return Err("Unsupported mathematical operation".to_string());
        };

        return Ok(
          serde_json::json!({
                    "result": result,
                    "worker_id": worker_id,
                    "optimization": "rust_mathematical"
                }).to_string()
        );
      }
    }

    Err("Complex mathematical function - needs JavaScript worker".to_string())
  }

  /// Optimized string operations
  fn execute_string_optimized(
    function_code: &str,
    data: &serde_json::Value
  ) -> std::result::Result<String, String> {
    let clean_fn = function_code.replace([' ', '\n', '\t'], "").to_lowercase();

    if let Some(s) = data.as_str() {
      let result = if clean_fn.contains("touppercase") {
        s.to_uppercase()
      } else if clean_fn.contains("tolowercase") {
        s.to_lowercase()
      } else if clean_fn.contains("length") {
        return Ok(s.len().to_string());
      } else if clean_fn.contains("reverse") {
        s.chars().rev().collect::<String>()
      } else if clean_fn.contains("trim") {
        s.trim().to_string()
      } else {
        return Err("Unsupported string operation".to_string());
      };

      return Ok(format!("\"{}\"", result));
    }

    Err("Complex string function - needs JavaScript worker".to_string())
  }

  /// Optimized array operations
  fn execute_array_optimized(
    function_code: &str,
    data: &serde_json::Value
  ) -> std::result::Result<String, String> {
    let clean_fn = function_code.replace([' ', '\n', '\t'], "").to_lowercase();

    if let Some(arr) = data.as_array() {
      if clean_fn.contains("length") {
        return Ok(arr.len().to_string());
      }

      // Simple array operations that can be done in Rust
      if clean_fn.contains("sum") && arr.iter().all(|v| v.is_number()) {
        let sum: f64 = arr
          .iter()
          .filter_map(|v| v.as_f64())
          .sum();
        return Ok(sum.to_string());
      }

      if clean_fn.contains("max") && arr.iter().all(|v| v.is_number()) {
        let max = arr
          .iter()
          .filter_map(|v| v.as_f64())
          .fold(f64::NEG_INFINITY, f64::max);
        return Ok(max.to_string());
      }

      if clean_fn.contains("min") && arr.iter().all(|v| v.is_number()) {
        let min = arr
          .iter()
          .filter_map(|v| v.as_f64())
          .fold(f64::INFINITY, f64::min);
        return Ok(min.to_string());
      }
    }

    Err("Complex array function - needs JavaScript worker".to_string())
  }

  /// General optimized execution fallback
  fn execute_general_optimized(
    function_code: &str,
    data: &serde_json::Value,
    worker_id: usize
  ) -> std::result::Result<String, String> {
    // Try mathematical operations first
    if let Ok(result) = Self::execute_mathematical_optimized(function_code, data, worker_id) {
      return Ok(result);
    }

    // Try string operations
    if let Ok(result) = Self::execute_string_optimized(function_code, data) {
      return Ok(result);
    }

    // Try array operations
    if let Ok(result) = Self::execute_array_optimized(function_code, data) {
      return Ok(result);
    }

    // Default fallback
    Err("General function requires JavaScript worker".to_string())
  }

  /// Update optimization statistics
  fn update_optimization_stats(&self, result: &OptimizedWorkerResult) {
    if let Ok(mut stats) = self.optimization_stats.lock() {
      stats.total_executions += 1;

      match result.optimization_used.as_str() {
        "rust_native" | "rust_optimized" => {
          stats.rust_native_hits += 1;
        }
        "needs_js_worker" => {
          stats.js_worker_fallbacks += 1;
        }
        _ => {}
      }

      if result.cache_hit {
        stats.cache_hits += 1;
      }

      // Update running averages
      let current_latency = stats.avg_latency_ms;
      let total = stats.total_executions as f64;
      stats.avg_latency_ms =
        (current_latency * (total - 1.0) + (result.duration_ms as f64)) / total;
    }
  }

  /// Generate unique task ID
  fn generate_task_id() -> String {
    use std::time::{ SystemTime, UNIX_EPOCH };
    let timestamp = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_nanos();
    format!("opt_task_{}", timestamp)
  }

  #[napi(getter)]
  pub fn worker_count(&self) -> u32 {
    self.worker_count as u32
  }

  #[napi]
  pub fn shutdown(&self) -> Result<()> {
    if let Ok(mut active) = self.workers_active.lock() {
      *active = false;
    }

    if let Ok(mut sender_guard) = self.task_sender.lock() {
      *sender_guard = None;
    }

    if let Ok(mut receiver_guard) = self.result_receiver.lock() {
      *receiver_guard = None;
    }

    Ok(())
  }
}

/// Check if optimized multi-core execution is available
#[napi]
pub fn is_optimized_multicore_available() -> bool {
  true
}

/// Get enhanced system information with optimization capabilities
#[napi]
pub fn get_optimized_multicore_info() -> Result<String> {
  let info =
    serde_json::json!({
        "cpu_cores": num_cpus::get(),
        "physical_cores": num_cpus::get_physical(),
        "rust_version": env!("CARGO_PKG_VERSION"),
        "optimization_engine": "Enhanced Rust + Node.js Workers with 2-Phase Pipeline",
        "features": [
            "multi_core_optimized",
            "rust_native_fast_path",
            "optimization_hints_support",
            "adaptive_batching",
            "hot_function_caching",
            "performance_learning",
            "binary_protocol_support",
            "jit_compilation_hints",
            "workload_specific_optimization"
        ],
        "optimization_capabilities": {
            "mathematical_operations": true,
            "string_operations": true, 
            "array_operations": true,
            "hot_function_detection": true,
            "performance_caching": true,
            "adaptive_routing": true
        }
    });

  Ok(info.to_string())
}
